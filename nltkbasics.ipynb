{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The focus of such an essay predicts its structure.', 'It dictates the information readers need to know and the order in which they need to receive it.', \"Thus your essay's structure is necessarily unique to the main claim you're making.\", 'Although there are guidelines for constructing certain classic essay types (e.g., comparative analysis), there are no set formula.']\n"
     ]
    }
   ],
   "source": [
    "example_txt=\"The focus of such an essay predicts its structure. It dictates the information readers need to know and the order in which they need to receive it. Thus your essay's structure is necessarily unique to the main claim you're making. Although there are guidelines for constructing certain classic essay types (e.g., comparative analysis), there are no set formula.\"\n",
    "print(sent_tokenize(example_txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(example_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"didn't\", 'an', 'hers', 'until', 'of', 'hasn', \"couldn't\", 'below', 'shouldn', 'because', \"haven't\", 'those', 'couldn', 'again', 'there', \"shan't\", 'your', 'don', 'under', 'itself', 're', 'above', 'other', 'such', 'with', 'what', 'then', 'didn', 'which', 'yourself', 'out', 'its', 'did', \"mustn't\", 'nor', 'having', 'had', \"you're\", \"that'll\", 'on', 'being', 'mightn', 'have', 'same', 'their', 'against', 'through', 'them', 'when', 'myself', 'theirs', 'am', 'her', 'doing', 'before', 'just', 'few', 'down', 'as', 'between', 'over', \"wasn't\", 'will', 'aren', \"don't\", 'that', 'most', 'needn', 'all', 's', 'yourselves', \"should've\", 'where', 'themselves', 'so', 'does', 'doesn', 'these', \"needn't\", 'why', \"hasn't\", \"wouldn't\", 'yours', 'ma', 'herself', 'can', 'wasn', 'or', 'in', 'a', 'was', 'has', 'once', 'but', 'y', \"doesn't\", 'we', 'any', 'ourselves', 'ours', 'is', 'it', 'll', 'who', \"you'd\", 'won', 'how', 'further', 'the', 'only', 'were', 'my', 'about', \"isn't\", 'they', 'this', \"hadn't\", 'to', 'our', \"she's\", 't', 'ain', 'into', 'now', 'each', 'hadn', 'you', 'do', \"it's\", 'up', 'for', 'very', 'more', 'than', 'i', \"you'll\", 'during', \"weren't\", 'o', 'me', \"won't\", 'she', \"aren't\", \"you've\", 'be', 'while', 'himself', 'mustn', 'weren', 'no', 'd', 'wouldn', \"shouldn't\", 'some', 'haven', 'not', \"mightn't\", 'been', 'by', 'at', 'here', 'his', 'too', 'should', 'shan', 'off', 've', 'isn', 'he', 'after', 'own', 'm', 'are', 'him', 'both', 'if', 'from', 'whom', 'and'}\n"
     ]
    }
   ],
   "source": [
    "# stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'focus', 'essay', 'predicts', 'structure', '.', 'It', 'dictates', 'information', 'readers', 'need', 'know', 'order', 'need', 'receive', '.', 'Thus', 'essay', \"'s\", 'structure', 'necessarily', 'unique', 'main', 'claim', \"'re\", 'making', '.', 'Although', 'guidelines', 'constructing', 'certain', 'classic', 'essay', 'types', '(', 'e.g.', ',', 'comparative', 'analysis', ')', ',', 'set', 'formula', '.']\n"
     ]
    }
   ],
   "source": [
    "# filtered_sentence=[]\n",
    "# for w in words:\n",
    "#     if w not in stop_words:\n",
    "#         filtered_sentence.append(w)\n",
    "# print(filtered_sentence)\n",
    "filtered_sentence=[w for w in words if w not in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "# stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "example_words=[\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos tagging\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt=state_union.raw(\"C:/Users/TAMIL/Desktop/New folder/aboutapj1.txt\")\n",
    "sample_txt=state_union.raw(\"C:/Users/TAMIL/Desktop/New folder/aboutapj.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sent_tokenizer=PunktSentenceTokenizer(train_txt)\n",
    "tokenized=custom_sent_tokenizer.tokenize(sample_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Avul', 'NNP'), ('Pakir', 'NNP'), ('Jainulabdeen', 'NNP'), ('Abdul', 'NNP'), ('Kalam', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('on', 'IN'), ('15', 'CD'), ('October', 'NNP'), ('1931', 'CD'), ('to', 'TO'), ('a', 'DT'), ('Tamil', 'NNP'), ('Muslim', 'NNP'), ('family', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pilgrimage', 'NN'), ('centre', 'NN'), ('of', 'IN'), ('Rameswaram', 'NNP'), ('on', 'IN'), ('Pamban', 'NNP'), ('Island', 'NNP'), (',', ','), ('then', 'RB'), ('in', 'IN'), ('the', 'DT'), ('Madras', 'NNP'), ('Presidency', 'NNP'), ('and', 'CC'), ('now', 'RB'), ('in', 'IN'), ('the', 'DT'), ('State', 'NNP'), ('of', 'IN'), ('Tamil', 'NNP'), ('Nadu', 'NNP'), ('.', '.')]\n",
      "[('His', 'PRP$'), ('father', 'NN'), ('Jainulabdeen', 'NNP'), ('was', 'VBD'), ('a', 'DT'), ('boat', 'NN'), ('owner', 'NN'), ('and', 'CC'), ('imam', 'NN'), ('of', 'IN'), ('a', 'DT'), ('local', 'JJ'), ('mosque', 'NN'), (';', ':'), ('[', 'CC'), ('9', 'CD'), (']', 'VBD'), ('his', 'PRP$'), ('mother', 'NN'), ('Ashiamma', 'NNP'), ('was', 'VBD'), ('a', 'DT'), ('housewife', 'NN'), ('.', '.')]\n",
      "[('[', 'RB'), ('10', 'CD'), (']', 'JJ'), ('[', '$'), ('11', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('12', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('13', 'CD'), (']', 'IN'), ('His', 'PRP$'), ('father', 'NN'), ('owned', 'VBD'), ('a', 'DT'), ('ferry', 'NN'), ('that', 'WDT'), ('took', 'VBD'), ('Hindu', 'NNP'), ('pilgrims', 'VBZ'), ('back', 'RB'), ('and', 'CC'), ('forth', 'NN'), ('between', 'IN'), ('Rameswaram', 'NNP'), ('and', 'CC'), ('the', 'DT'), ('now', 'RB'), ('uninhabited', 'JJ'), ('Dhanushkodi', 'NNP'), ('.', '.')]\n",
      "[('[', 'RB'), ('14', 'CD'), (']', 'JJ'), ('[', '$'), ('15', 'CD'), (']', 'NNP'), ('Kalam', 'NNP'), ('was', 'VBD'), ('the', 'DT'), ('youngest', 'JJS'), ('of', 'IN'), ('four', 'CD'), ('brothers', 'NNS'), ('and', 'CC'), ('one', 'CD'), ('sister', 'NN'), ('in', 'IN'), ('his', 'PRP$'), ('family', 'NN'), ('.', '.')]\n",
      "[('[', 'RB'), ('16', 'CD'), (']', 'JJ'), ('[', '$'), ('17', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('18', 'CD'), (']', 'IN'), ('His', 'PRP$'), ('ancestors', 'NNS'), ('had', 'VBD'), ('been', 'VBN'), ('wealthy', 'JJ'), ('traders', 'NNS'), ('and', 'CC'), ('landowners', 'NNS'), (',', ','), ('with', 'IN'), ('numerous', 'JJ'), ('properties', 'NNS'), ('and', 'CC'), ('large', 'JJ'), ('tracts', 'NNS'), ('of', 'IN'), ('land', 'NN'), ('.', '.')]\n",
      "[('Their', 'PRP$'), ('business', 'NN'), ('had', 'VBD'), ('involved', 'VBN'), ('trading', 'NN'), ('groceries', 'NNS'), ('between', 'IN'), ('the', 'DT'), ('mainland', 'NN'), ('and', 'CC'), ('the', 'DT'), ('island', 'NN'), ('and', 'CC'), ('to', 'TO'), ('and', 'CC'), ('from', 'IN'), ('Sri', 'NNP'), ('Lanka', 'NNP'), (',', ','), ('as', 'RB'), ('well', 'RB'), ('as', 'IN'), ('ferrying', 'VBG'), ('pilgrims', 'NNS'), ('between', 'IN'), ('the', 'DT'), ('mainland', 'NN'), ('and', 'CC'), ('Pamban', 'NNP'), ('.', '.')]\n",
      "[('As', 'IN'), ('a', 'DT'), ('result', 'NN'), (',', ','), ('the', 'DT'), ('family', 'NN'), ('acquired', 'VBD'), ('the', 'DT'), ('title', 'NN'), ('of', 'IN'), ('``', '``'), ('Mara', 'NNP'), ('Kalam', 'NNP'), ('Iyakkivar', 'NNP'), (\"''\", \"''\"), ('(', '('), ('wooden', 'JJ'), ('boat', 'NN'), ('steerers', 'NNS'), (')', ')'), (',', ','), ('which', 'WDT'), ('over', 'IN'), ('the', 'DT'), ('years', 'NNS'), ('became', 'VBD'), ('shortened', 'VBD'), ('to', 'TO'), ('``', '``'), ('Marakier', 'NNP'), ('.', '.'), (\"''\", \"''\")]\n",
      "[('With', 'IN'), ('the', 'DT'), ('opening', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Pamban', 'NNP'), ('Bridge', 'NNP'), ('to', 'TO'), ('the', 'DT'), ('mainland', 'NN'), ('in', 'IN'), ('1914', 'CD'), (',', ','), ('however', 'RB'), (',', ','), ('the', 'DT'), ('businesses', 'NNS'), ('failed', 'VBD'), ('and', 'CC'), ('the', 'DT'), ('family', 'NN'), ('fortune', 'NN'), ('and', 'CC'), ('properties', 'NNS'), ('were', 'VBD'), ('lost', 'VBN'), ('over', 'IN'), ('time', 'NN'), (',', ','), ('apart', 'RB'), ('from', 'IN'), ('the', 'DT'), ('ancestral', 'JJ'), ('home', 'NN'), ('.', '.')]\n",
      "[('[', 'RB'), ('19', 'CD'), (']', 'NNS'), ('By', 'IN'), ('his', 'PRP$'), ('early', 'JJ'), ('childhood', 'NN'), (',', ','), ('Kalam', 'NNP'), (\"'s\", 'POS'), ('family', 'NN'), ('had', 'VBD'), ('become', 'VBN'), ('poor', 'JJ'), (';', ':'), ('at', 'IN'), ('an', 'DT'), ('early', 'JJ'), ('age', 'NN'), (',', ','), ('he', 'PRP'), ('sold', 'VBD'), ('newspapers', 'NNS'), ('to', 'TO'), ('supplement', 'VB'), ('his', 'PRP$'), ('family', 'NN'), (\"'s\", 'POS'), ('income', 'NN'), ('.', '.')]\n",
      "[('[', 'RB'), ('20', 'CD'), (']', 'JJ'), ('[', '$'), ('20', 'CD'), (']', 'NNP'), ('[', 'VBD'), ('21', 'CD'), (']', 'NN'), ('In', 'IN'), ('his', 'PRP$'), ('school', 'NN'), ('years', 'NNS'), (',', ','), ('Kalam', 'NNP'), ('had', 'VBD'), ('average', 'JJ'), ('grades', 'NNS'), ('but', 'CC'), ('was', 'VBD'), ('described', 'VBN'), ('as', 'IN'), ('a', 'DT'), ('bright', 'JJ'), ('and', 'CC'), ('hardworking', 'JJ'), ('student', 'NN'), ('who', 'WP'), ('had', 'VBD'), ('a', 'DT'), ('strong', 'JJ'), ('desire', 'NN'), ('to', 'TO'), ('learn', 'VB'), ('.', '.')]\n",
      "[('He', 'PRP'), ('spent', 'VBD'), ('hours', 'NNS'), ('on', 'IN'), ('his', 'PRP$'), ('studies', 'NNS'), (',', ','), ('especially', 'RB'), ('mathematics', 'NNS'), ('.', '.')]\n",
      "[('[', 'RB'), ('21', 'CD'), (']', 'NNS'), ('After', 'IN'), ('completing', 'VBG'), ('his', 'PRP$'), ('education', 'NN'), ('at', 'IN'), ('the', 'DT'), ('Schwartz', 'NNP'), ('Higher', 'NNP'), ('Secondary', 'NNP'), ('School', 'NNP'), (',', ','), ('Ramanathapuram', 'NNP'), (',', ','), ('Kalam', 'NNP'), ('went', 'VBD'), ('on', 'IN'), ('to', 'TO'), ('attend', 'VB'), ('Saint', 'NNP'), ('Joseph', 'NNP'), (\"'s\", 'POS'), ('College', 'NNP'), (',', ','), ('Tiruchirappalli', 'NNP'), (',', ','), ('then', 'RB'), ('affiliated', 'VBD'), ('with', 'IN'), ('the', 'DT'), ('University', 'NNP'), ('of', 'IN'), ('Madras', 'NNP'), (',', ','), ('from', 'IN'), ('where', 'WRB'), ('he', 'PRP'), ('graduated', 'VBD'), ('in', 'IN'), ('physics', 'NNS'), ('in', 'IN'), ('1954', 'CD'), ('.', '.')]\n",
      "[('[', 'RB'), ('22', 'CD'), (']', 'NN'), ('He', 'PRP'), ('moved', 'VBD'), ('to', 'TO'), ('Madras', 'NNP'), ('in', 'IN'), ('1955', 'CD'), ('to', 'TO'), ('study', 'VB'), ('aerospace', 'NN'), ('engineering', 'NN'), ('in', 'IN'), ('Madras', 'NNP'), ('Institute', 'NNP'), ('of', 'IN'), ('Technology', 'NNP'), ('.', '.')]\n",
      "[('[', 'RB'), ('13', 'CD'), (']', 'NNS'), ('While', 'IN'), ('Kalam', 'NNP'), ('was', 'VBD'), ('working', 'VBG'), ('on', 'IN'), ('a', 'DT'), ('senior', 'JJ'), ('class', 'NN'), ('project', 'NN'), (',', ','), ('the', 'DT'), ('Dean', 'NNP'), ('was', 'VBD'), ('dissatisfied', 'VBN'), ('with', 'IN'), ('his', 'PRP$'), ('lack', 'NN'), ('of', 'IN'), ('progress', 'NN'), ('and', 'CC'), ('threatened', 'VBD'), ('to', 'TO'), ('revoke', 'VB'), ('his', 'PRP$'), ('scholarship', 'NN'), ('unless', 'IN'), ('the', 'DT'), ('project', 'NN'), ('was', 'VBD'), ('finished', 'VBN'), ('within', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('three', 'CD'), ('days', 'NNS'), ('.', '.')]\n",
      "[('Kalam', 'NNP'), ('met', 'VBD'), ('the', 'DT'), ('deadline', 'NN'), (',', ','), ('impressing', 'VBG'), ('the', 'DT'), ('Dean', 'NNP'), (',', ','), ('who', 'WP'), ('later', 'RB'), ('said', 'VBD'), ('to', 'TO'), ('him', 'PRP'), (',', ','), ('``', '``'), ('I', 'PRP'), ('was', 'VBD'), ('putting', 'VBG'), ('you', 'PRP'), ('under', 'IN'), ('stress', 'NN'), ('and', 'CC'), ('asking', 'VBG'), ('you', 'PRP'), ('to', 'TO'), ('meet', 'VB'), ('a', 'DT'), ('difficult', 'JJ'), ('deadline', 'NN'), (\"''\", \"''\"), ('.', '.')]\n",
      "[('[', 'RB'), ('23', 'CD'), (']', 'NN'), ('He', 'PRP'), ('narrowly', 'RB'), ('missed', 'VBD'), ('achieving', 'VBG'), ('his', 'PRP$'), ('dream', 'NN'), ('of', 'IN'), ('becoming', 'VBG'), ('a', 'DT'), ('fighter', 'NN'), ('pilot', 'NN'), (',', ','), ('as', 'IN'), ('he', 'PRP'), ('placed', 'VBD'), ('ninth', 'JJ'), ('in', 'IN'), ('qualifiers', 'NNS'), (',', ','), ('and', 'CC'), ('only', 'RB'), ('eight', 'CD'), ('positions', 'NNS'), ('were', 'VBD'), ('available', 'JJ'), ('in', 'IN'), ('the', 'DT'), ('IAF', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words=nltk.word_tokenize(i)\n",
    "            tagged=nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk Avul/NNP)\n",
      "  (Chunk Pakir/NNP)\n",
      "  (Chunk Jainulabdeen/NNP)\n",
      "  (Chunk Abdul/NNP)\n",
      "  (Chunk Kalam/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  on/IN\n",
      "  15/CD\n",
      "  (Chunk October/NNP)\n",
      "  1931/CD\n",
      "  to/TO\n",
      "  a/DT\n",
      "  (Chunk Tamil/NNP)\n",
      "  (Chunk Muslim/NNP family/NN)\n",
      "  in/IN\n",
      "  the/DT\n",
      "  pilgrimage/NN\n",
      "  centre/NN\n",
      "  of/IN\n",
      "  (Chunk Rameswaram/NNP)\n",
      "  on/IN\n",
      "  (Chunk Pamban/NNP)\n",
      "  (Chunk Island/NNP)\n",
      "  ,/,\n",
      "  then/RB\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (Chunk Madras/NNP)\n",
      "  (Chunk Presidency/NNP)\n",
      "  and/CC\n",
      "  now/RB\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (Chunk State/NNP)\n",
      "  of/IN\n",
      "  (Chunk Tamil/NNP)\n",
      "  (Chunk Nadu/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  His/PRP$\n",
      "  father/NN\n",
      "  (Chunk Jainulabdeen/NNP)\n",
      "  was/VBD\n",
      "  a/DT\n",
      "  boat/NN\n",
      "  owner/NN\n",
      "  and/CC\n",
      "  imam/NN\n",
      "  of/IN\n",
      "  a/DT\n",
      "  local/JJ\n",
      "  mosque/NN\n",
      "  ;/:\n",
      "  [/CC\n",
      "  9/CD\n",
      "  ]/VBD\n",
      "  his/PRP$\n",
      "  mother/NN\n",
      "  (Chunk Ashiamma/NNP)\n",
      "  was/VBD\n",
      "  a/DT\n",
      "  housewife/NN\n",
      "  ./.)\n",
      "(S\n",
      "  [/RB\n",
      "  10/CD\n",
      "  ]/JJ\n",
      "  [/$\n",
      "  11/CD\n",
      "  (Chunk ]/NNP)\n",
      "  [/VBD\n",
      "  12/CD\n",
      "  (Chunk ]/NNP)\n",
      "  [/VBD\n",
      "  13/CD\n",
      "  ]/IN\n",
      "  His/PRP$\n",
      "  father/NN\n",
      "  owned/VBD\n",
      "  a/DT\n",
      "  ferry/NN\n",
      "  that/WDT\n",
      "  (Chunk took/VBD Hindu/NNP)\n",
      "  pilgrims/VBZ\n",
      "  back/RB\n",
      "  and/CC\n",
      "  forth/NN\n",
      "  between/IN\n",
      "  (Chunk Rameswaram/NNP)\n",
      "  and/CC\n",
      "  the/DT\n",
      "  now/RB\n",
      "  uninhabited/JJ\n",
      "  (Chunk Dhanushkodi/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  [/RB\n",
      "  14/CD\n",
      "  ]/JJ\n",
      "  [/$\n",
      "  15/CD\n",
      "  (Chunk ]/NNP)\n",
      "  (Chunk Kalam/NNP)\n",
      "  was/VBD\n",
      "  the/DT\n",
      "  youngest/JJS\n",
      "  of/IN\n",
      "  four/CD\n",
      "  brothers/NNS\n",
      "  and/CC\n",
      "  one/CD\n",
      "  sister/NN\n",
      "  in/IN\n",
      "  his/PRP$\n",
      "  family/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# chunking\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words=nltk.word_tokenize(i)\n",
    "            tagged=nltk.pos_tag(words)\n",
    "            chunkGram=r\"\"\"Chunk:{<RB.?>*<VB.?>*<NNP><NN>?}\"\"\"\n",
    "            chunkParser=nltk.RegexpParser(chunkGram)\n",
    "            chunked=chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chinking\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words=nltk.word_tokenize(i)\n",
    "            tagged=nltk.pos_tag(words)\n",
    "            chunkGram=r\"\"\"Chunk:{<.*>+}\n",
    "                                }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "            chunkParser=nltk.RegexpParser(chunkGram)\n",
    "            chunked=chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            chunked.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nammmed entity\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words=nltk.word_tokenize(i)\n",
    "            tagged=nltk.pos_tag(words)\n",
    "            named_ent=nltk.ne_chunk(tagged,binary=True)\n",
    "            print(named_ent)\n",
    "            named_ent.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "# default position is noun\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "# position adjective\n",
    "print(lemmatizer.lemmatize(\"better\",pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\",pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "# position verb\n",
    "print(lemmatizer.lemmatize(\"run\",\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpora\n",
    "# corpus data search for %appdata% ->nltk_data\n",
    "from nltk.corpus import abc\n",
    "sample=abc.raw(\"science.txt\")\n",
    "tokens=sent_tokenize(sample)\n",
    "print(tokens[1:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordnet\n",
    "# synset - interface to look up words in wordnet \n",
    "from nltk.corpus import wordnet\n",
    "syns=wordnet.synsets(\"great\")\n",
    "\n",
    "# whole synset\n",
    "print(syns[0].name())\n",
    "\n",
    "# only the word\n",
    "print(syns[0].lemmas()[0].name())\n",
    "\n",
    "# definition\n",
    "print(syns[0].definition())\n",
    "\n",
    "# examples\n",
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synonyms and antonyms using wordnet\n",
    "synonyms=[]\n",
    "antonyms=[]\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic similarity between words\n",
    "w1=wordnet.synset(\"ship.n.01\")\n",
    "w2=wordnet.synset(\"boat.n.01\")\n",
    "print(w1.wup_similarity(w2))\n",
    "\n",
    "w1=wordnet.synset(\"ship.n.01\")\n",
    "w2=wordnet.synset(\"car.n.01\")\n",
    "print(w1.wup_similarity(w2))\n",
    "\n",
    "w1=wordnet.synset(\"ship.n.01\")\n",
    "w2=wordnet.synset(\"cactus.n.01\")\n",
    "print(w1.wup_similarity(w2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
